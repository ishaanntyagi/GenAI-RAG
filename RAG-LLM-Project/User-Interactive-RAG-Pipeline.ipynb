{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# User-Interactive RAG Pipeline\n",
    "\n",
    "This notebook provides a complete end-to-end RAG (Retrieval-Augmented Generation) pipeline that:\n",
    "1. Accepts user input for PDF file path\n",
    "2. Asks for Google API key\n",
    "3. Follows the same pipeline as the existing notebooks\n",
    "4. Provides an interactive Q&A interface\n",
    "\n",
    "## Required Libraries\n",
    "Make sure you have the following libraries installed:\n",
    "- `pdfplumber`\n",
    "- `google-generativeai`\n",
    "- `sentence-transformers`\n",
    "- `chromadb`\n",
    "- `pickle`\n",
    "- `os`\n",
    "- `warnings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pdfplumber\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "user_inputs",
   "metadata": {},
   "source": [
    "## User Input Section\n",
    "Please provide the required inputs below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get_pdf_path",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PDF file path from user\n",
    "print(\"=== PDF File Input ===\")\n",
    "pdf_path = input(\"Please enter the full path to your PDF file: \").strip()\n",
    "\n",
    "# Validate if the file exists\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Error: File not found at {pdf_path}\")\n",
    "    print(\"Please check the path and try again.\")\n",
    "else:\n",
    "    print(f\"‚úì PDF file found: {pdf_path}\")\n",
    "    \n",
    "# Check if it's a PDF file\n",
    "if not pdf_path.lower().endswith('.pdf'):\n",
    "    print(\"Warning: The file doesn't appear to be a PDF. Proceeding anyway...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get_api_key",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Google API key from user\n",
    "print(\"\\n=== Google API Key Input ===\")\n",
    "print(\"Please enter your Google Gemini API key.\")\n",
    "print(\"You can get one from: https://makersuite.google.com/app/apikey\")\n",
    "api_key = input(\"Enter your Google API key: \").strip()\n",
    "\n",
    "if not api_key:\n",
    "    print(\"Error: API key cannot be empty!\")\n",
    "else:\n",
    "    print(\"‚úì API key received\")\n",
    "    # Configure the Google AI API\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"‚úì Google AI API configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_processing",
   "metadata": {},
   "source": [
    "## Step 1: PDF Text Extraction and Chunking\n",
    "Extract text from the PDF and split it into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract_text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF (following the same approach as 01-Data-Chunking.ipynb)\n",
    "print(\"=== Extracting Text from PDF ===\")\n",
    "all_text = \" \"\n",
    "\n",
    "try:\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        total_pages = len(pdf.pages)\n",
    "        print(f\"Processing {total_pages} pages...\")\n",
    "        \n",
    "        for i, page in enumerate(pdf.pages, 1):\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                all_text += page_text + \"\\n\"\n",
    "                if i % 10 == 0:  # Progress update every 10 pages\n",
    "                    print(f\"Processed {i}/{total_pages} pages\")\n",
    "                    \n",
    "    print(f\"‚úì Successfully extracted text from {total_pages} pages\")\n",
    "    print(f\"Total text length: {len(all_text)} characters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error extracting text from PDF: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunk_text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks (following the same approach as existing notebooks)\n",
    "print(\"\\n=== Chunking Text ===\")\n",
    "\n",
    "# Simple chunking by paragraphs and sentences\n",
    "# This follows the same pattern as the original notebooks\n",
    "def chunk_text(text, max_chunk_size=1000):\n",
    "    \"\"\"Split text into chunks of approximately max_chunk_size characters.\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Split by double newlines first (paragraphs)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    current_chunk = \"\"\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph = paragraph.strip()\n",
    "        if not paragraph:\n",
    "            continue\n",
    "            \n",
    "        # If adding this paragraph would exceed max_chunk_size, save current chunk\n",
    "        if len(current_chunk) + len(paragraph) > max_chunk_size and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = paragraph\n",
    "        else:\n",
    "            current_chunk += (\" \" if current_chunk else \"\") + paragraph\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create chunks\n",
    "chunks = chunk_text(all_text)\n",
    "print(f\"‚úì Created {len(chunks)} chunks\")\n",
    "print(f\"Average chunk size: {sum(len(chunk) for chunk in chunks) // len(chunks)} characters\")\n",
    "\n",
    "# Show a sample chunk\n",
    "if chunks:\n",
    "    print(f\"\\nSample chunk (first 200 chars): {chunks[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embeddings",
   "metadata": {},
   "source": [
    "## Step 2: Generate Embeddings\n",
    "Create embeddings for all text chunks using SentenceTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings (following the same approach as 02-Embeddings.ipynb)\n",
    "print(\"=== Creating Embeddings ===\")\n",
    "print(\"Loading SentenceTransformer model...\")\n",
    "\n",
    "# Load the same model as used in the original notebooks\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"‚úì SentenceTransformer model loaded\")\n",
    "\n",
    "print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
    "embeddings = model.encode(chunks)\n",
    "print(f\"‚úì Generated embeddings with shape: {embeddings.shape}\")\n",
    "\n",
    "# Save embeddings and chunks for future use (optional)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "chunks_filename = f\"user_chunks_{timestamp}.pkl\"\n",
    "embeddings_filename = f\"user_embeddings_{timestamp}.pkl\"\n",
    "\n",
    "with open(chunks_filename, 'wb') as f:\n",
    "    pickle.dump(chunks, f)\n",
    "    \n",
    "with open(embeddings_filename, 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "    \n",
    "print(f\"‚úì Saved chunks to {chunks_filename}\")\n",
    "print(f\"‚úì Saved embeddings to {embeddings_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vector_db",
   "metadata": {},
   "source": [
    "## Step 3: Store in Vector Database\n",
    "Store the embeddings in ChromaDB for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_chromadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ChromaDB (following the same approach as 04-VectorDB-Chroma.ipynb)\n",
    "print(\"=== Setting up ChromaDB ===\")\n",
    "\n",
    "# Create a unique database path for this session\n",
    "db_path = f\"user_chroma_db_{timestamp}\"\n",
    "chroma_client = chromadb.PersistentClient(path=db_path)\n",
    "print(f\"‚úì ChromaDB client created with path: {db_path}\")\n",
    "\n",
    "# Create collection\n",
    "collection_name = f\"user_collection_{timestamp}\"\n",
    "collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "print(f\"‚úì Collection '{collection_name}' created\")\n",
    "\n",
    "# Add documents to the collection\n",
    "print(f\"Adding {len(chunks)} documents to the collection...\")\n",
    "\n",
    "# Prepare data for ChromaDB\n",
    "ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "metadatas = [{\"chunk_id\": i, \"source\": os.path.basename(pdf_path)} for i in range(len(chunks))]\n",
    "\n",
    "# Add to collection\n",
    "collection.add(\n",
    "    embeddings=embeddings.tolist(),\n",
    "    documents=chunks,\n",
    "    ids=ids,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "\n",
    "print(f\"‚úì Successfully added {collection.count()} documents to ChromaDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval_qa",
   "metadata": {},
   "source": [
    "## Step 4: Interactive Q&A System\n",
    "Query the system with questions and get AI-powered responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_qa_system",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Q&A system (following the same approach as 05-LLM-API-Retrieval.ipynb)\n",
    "print(\"=== Setting up Q&A System ===\")\n",
    "\n",
    "def answer_question(question, num_results=3):\n",
    "    \"\"\"Answer a question using the RAG pipeline.\"\"\"\n",
    "    \n",
    "    # Generate embedding for the question\n",
    "    question_embedding = model.encode(question)\n",
    "    \n",
    "    # Query the vector database\n",
    "    results = collection.query(\n",
    "        query_embeddings=[question_embedding],\n",
    "        n_results=num_results\n",
    "    )\n",
    "    \n",
    "    # Get the retrieved chunks\n",
    "    retrieved_chunks = results['documents'][0]\n",
    "    \n",
    "    # Create context from retrieved chunks\n",
    "    context = \"\\n---\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    # Create prompt for the LLM\n",
    "    prompt = f\"\"\"Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Generate response using Google Gemini\n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        \n",
    "        return {\n",
    "            'answer': response.text,\n",
    "            'retrieved_chunks': retrieved_chunks,\n",
    "            'context': context\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'error': f\"Error generating response: {str(e)}\",\n",
    "            'retrieved_chunks': retrieved_chunks,\n",
    "            'context': context\n",
    "        }\n",
    "\n",
    "print(\"‚úì Q&A system ready!\")\n",
    "print(\"\\nYou can now ask questions about your PDF document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive_qa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Q&A loop\n",
    "print(\"=== Interactive Q&A Session ===\")\n",
    "print(\"Ask questions about your PDF document. Type 'quit' to exit.\")\n",
    "print(\"Type 'help' for available commands.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "while True:\n",
    "    question = input(\"\\nYour question: \").strip()\n",
    "    \n",
    "    if question.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"Thank you for using the RAG pipeline!\")\n",
    "        break\n",
    "    \n",
    "    if question.lower() == 'help':\n",
    "        print(\"Available commands:\")\n",
    "        print(\"- Type any question about your PDF document\")\n",
    "        print(\"- 'quit' or 'exit' or 'q' to exit\")\n",
    "        print(\"- 'help' to see this message\")\n",
    "        print(\"- 'stats' to see system statistics\")\n",
    "        continue\n",
    "    \n",
    "    if question.lower() == 'stats':\n",
    "        print(f\"System Statistics:\")\n",
    "        print(f\"- PDF file: {os.path.basename(pdf_path)}\")\n",
    "        print(f\"- Total chunks: {len(chunks)}\")\n",
    "        print(f\"- Vector database: {collection.count()} documents\")\n",
    "        print(f\"- Collection name: {collection_name}\")\n",
    "        continue\n",
    "    \n",
    "    if not question:\n",
    "        print(\"Please enter a question.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing question: {question}\")\n",
    "    print(\"Searching for relevant information...\")\n",
    "    \n",
    "    # Get answer\n",
    "    result = answer_question(question)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"\\n‚ùå Error: {result['error']}\")\n",
    "        print(\"\\nRetrieved context for reference:\")\n",
    "        print(result['context'][:500] + \"...\")\n",
    "    else:\n",
    "        print(f\"\\nü§ñ Answer: {result['answer']}\")\n",
    "        \n",
    "        # Optionally show retrieved chunks\n",
    "        show_chunks = input(\"\\nShow retrieved chunks? (y/n): \").strip().lower()\n",
    "        if show_chunks == 'y':\n",
    "            print(\"\\nüìÑ Retrieved chunks:\")\n",
    "            for i, chunk in enumerate(result['retrieved_chunks'], 1):\n",
    "                print(f\"\\nChunk {i}:\")\n",
    "                print(chunk[:300] + \"...\" if len(chunk) > 300 else chunk)\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_sample_questions",
   "metadata": {},
   "source": [
    "## Test with Sample Questions\n",
    "If you want to test the system with some sample questions, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample_questions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample questions\n",
    "print(\"=== Testing with Sample Questions ===\")\n",
    "\n",
    "sample_questions = [\n",
    "    \"What is the main topic of this document?\",\n",
    "    \"Can you summarize the key points?\",\n",
    "    \"What are the most important concepts mentioned?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(sample_questions, 1):\n",
    "    print(f\"\\n{i}. Question: {question}\")\n",
    "    print(\"   Processing...\")\n",
    "    \n",
    "    result = answer_question(question)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"   ‚ùå Error: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"   ü§ñ Answer: {result['answer'][:200]}...\")\n",
    "    \n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "Remove temporary files if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup_files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cleanup\n",
    "import shutil\n",
    "\n",
    "cleanup = input(\"Do you want to remove temporary files? (y/n): \").strip().lower()\n",
    "\n",
    "if cleanup == 'y':\n",
    "    try:\n",
    "        # Remove pickle files\n",
    "        if os.path.exists(chunks_filename):\n",
    "            os.remove(chunks_filename)\n",
    "            print(f\"‚úì Removed {chunks_filename}\")\n",
    "        \n",
    "        if os.path.exists(embeddings_filename):\n",
    "            os.remove(embeddings_filename)\n",
    "            print(f\"‚úì Removed {embeddings_filename}\")\n",
    "        \n",
    "        # Remove ChromaDB directory\n",
    "        if os.path.exists(db_path):\n",
    "            shutil.rmtree(db_path)\n",
    "            print(f\"‚úì Removed {db_path}\")\n",
    "        \n",
    "        print(\"‚úì Cleanup completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup: {str(e)}\")\n",
    "else:\n",
    "    print(\"Temporary files preserved:\")\n",
    "    print(f\"- Chunks: {chunks_filename}\")\n",
    "    print(f\"- Embeddings: {embeddings_filename}\")\n",
    "    print(f\"- ChromaDB: {db_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}